# -*- coding: utf-8 -*-
"""behaviouralCloningCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R_kYerqGUEtBhAlXH2JnkVhh_Gy4sHps
"""

# Import libraries.

import os 
import csv
import cv2
import pickle
import numpy as np
import h5py as hp 
import pandas as pd
import tensorflow as tf 
import matplotlib.pyplot as plt 
import matplotlib.image as img 

from numpy import newaxis 
from keras import models 
from keras import layers 
from keras import optimizers 
from keras import regularizers 
from keras.models import Model 
from keras.optimizers import Adam
from keras.utils import to_categorical 
from sklearn.metrics import accuracy_score
from keras.applications.xception import Xception
from sklearn.model_selection import train_test_split  
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.inception_v3 import preprocess_input
from keras.layers import GlobalAveragePooling2D, MaxPooling2D, Dense, Dropout 
from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler

from google.colab import drive 
drive.mount('/content/drive') 

# Locations of folders in directory
dirName = "drive/My Drive/automatedDriving"

# Verify we are using GPU (in Tensorflow)
tf.test.gpu_device_name()

# Imported numpy data files.

X = np.load(dirName + '/' + 'X.npy')
y = np.load(dirName + '/' + 'y.npy')

print(len(y))
print('Data Loaded.')

def plot_img(dirName, img, title):

    fig = plt.figure(figsize = (10,10))
    plt.imshow(X[725,:,:])
    plt.axis('off')
    plt.show()

    fig.savefig(dirName + '/' + title + '.png', bbox_inches='tight')

    return
  
img = X[725,:,:]

plot_img(dirName, img, 'cnnImg')

# Plot the distribution of steering angles.
xPlot = range(len(y))

plt.figure(figsize=(20, 8))
plt.xlim(0,len(y))
plt.title('Data Distribution', fontsize=17)
plt.xlabel('Frames')
plt.ylabel('Steering Angle')
plt.plot(xPlot, y, 'g', linewidth=0.4)

plt.show()

# Load training, testing and validation datasets.
X_train = np.load(dirName + '/X_train.npy')
X_test = np.load(dirName + '/X_test.npy')
X_val = np.load(dirName + '/X_val.npy')

y_train = np.load(dirName + '/y_train.npy')
y_test = np.load(dirName + '/y_test.npy')
y_val = np.load(dirName + '/y_val.npy')

print('Data loaded.')

# Define and configure ImageDataGenerator for training data.
train_datagen = ImageDataGenerator(
    width_shift_range = 0.20)

# Define and configure ImageDataGenerator for validation data.
val_datagen = ImageDataGenerator()

#'''
# Code for displaying an array of augemented images from train_datagen.

# Directory to save augmented images.
#aug_imgs = "drive/My Drive/cs680_aug_images"

# for X_batch, y_batch in train_datagen.flow(X_train, y_train,
#                                            batch_size = 9):
  
# 	# create a grid of 3x3 images
# 	for i in range(0, 9):
# 		plt.subplot(320+i+1)
    
# 		plt.imshow(X_batch[i].reshape(80, 320, 3))
    
                       
# 	# show the plot
  
# 	plt.show()

# break
# #'''

# Define the batch size by calling the flow function.
train_generator = train_datagen.flow(X_train , y_train , batch_size = 64)
val_generator = val_datagen.flow(X_val , y_val , batch_size = 64)

################################################################################
#
# Continuous (linear regression) prediction of steering angles.
#
################################################################################

# Modified NVIDIA CNN model.

model = models.Sequential()

model.add(layers.Lambda(lambda x: x, input_shape=(80,320,3)))
model.add(layers.Conv2D(24, 5, 5, activation='relu', subsample=(2, 2)))
model.add(layers.Conv2D(36, 5, 5, activation='relu', subsample=(2, 2)))
model.add(layers.Conv2D(48, 5, 5, activation='relu', subsample=(2, 2)))
model.add(layers.Conv2D(64, 3, 3, activation='relu'))
model.add(layers.Conv2D(64, 3, 3, activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(100, activation='relu'))
model.add(layers.Dense(50, activation='relu'))
model.add(layers.Dense(10, activation='relu'))
model.add(layers.Dense(1, activation = 'linear'))

model.compile(optimizer = optimizers.RMSprop(lr=1e-5),
              loss='mean_squared_error')

model.summary()

# Checkpoint

parameterPath = dirName + "/nvidiaLinearRegression/" + "model_{epoch:02d}-{val_loss:.5f}.h5"

checkpoint = ModelCheckpoint(parameterPath,
                             monitor='val_loss',
                             verbose=0,
                             save_best_only = True,
                             mode='auto')

callbacks_list = [checkpoint]

with tf.device('/device:GPU:0'):
    history = model.fit_generator(train_generator,
                                  steps_per_epoch = len(X_train),
                                  validation_steps = 50,
                                  validation_data = val_generator,
                                  shuffle = True,
                                  epochs = 5,
                                  callbacks = callbacks_list,
                                  verbose = 1)

print('Steering angle predictions:')
preds = model.predict(X_test)

# Compute the difference between the *predicted* values and the
# *actual* values, then compute the percentage difference and
# the absolute percentage difference
diff = (preds.flatten()*0.6) - y_test

percentDiff = np.divide(diff , y_test) * 100

MP = np.mean(percentDiff)

absPercentDiff = np.abs(percentDiff)
 
# Compute the mean and standard deviation of the absolute percentage
# difference
mean = np.mean(absPercentDiff)
std = np.std(absPercentDiff)

print('Mean abs. percent difference:', mean)
print('Standard dev. of abs. percent difference:', std)
print('Mean percentage', MP)

# Extract loss for plotting.
loss_model = history.history['loss']
val_loss_model = history.history['val_loss']

epochs = range(1, len(loss_model) + 1)

plt.figure()

ax1.tick_params(color='white', labelcolor='white')

plt.plot(epochs, loss_model , 'm', label='Training Loss')
plt.plot(epochs, val_loss_model , 'b', label='Validation Loss')
plt.title('Training and Validation Loss for \n Linear Regression by NVIDIA Network' , size = 20, color='white')
plt.xlabel('Number of Epochs' , size = 15, color='white')
plt.ylabel('Loss' , size = 15, color='white')
plt.legend()

plt.show()

################################################################################
#
# Categorical (logistic regression) prediction of steering angles.
#
################################################################################

def calc_metrics(label , pred):
  '''
  Function to calculate differment measures
  commonly employed for machine learning.
  '''
  TP = TN = FP = FN = 0
    
  for true_label , pred_label in zip(label , pred):
    if true_label == pred_label:
      if true_label == 1:
        TP += 1
      else:
        TN += 1
    else:
      if true_label == 1:
        FP += 1
      else:
        FN += 1
                
  assert((TP + FP + TN + FN) == len(pred))
  
  # Calculate the desired quantities.
  precision = float(TP)/(TP + FP)
  recall = float(TP)/(TP + FN)
  accuracy = (float(TP + TN))/(TP + FP + TN + FN)
  sensitivity = float(TP)/(TP + FN)
  specificity = float(TN)/(TN + FP)
    
    
  return precision , recall , accuracy , sensitivity , specificity

import pandas as pd

# Number of desired bins -> Number of labels.
num_bins = 200

# Corresponding intervals for the number of bins.
interval = 2.0/num_bins

# Initialize increment at inc equal to the lower bound and bins as an empty list
# then append interval to inc until the upper bound is passed.
inc = -1
bins = []
while inc <= 1:
  inc = round(inc, 2)
  bins.append(inc)
  inc += interval

# Label vector.
labels = np.arange(0, len(bins) - 1)

# Bin the y-values of yAll into yBinned.
y_train = pd.cut(y_train, bins=bins, labels=labels, include_lowest=True)
y_test = pd.cut(y_test, bins=bins, labels=labels, include_lowest=True)
y_val = pd.cut(y_val, bins=bins, labels=labels, include_lowest=True)

def one_hot_vectors(data, nb_classes):
    """Convert an iterable of indices to one-hot encoded labels."""
    targets = np.array(data).reshape(-1)

    y = np.eye(nb_classes)[targets]

    return y

# Generate one-hot encoded vectors for training, testing, and validation.
y_train = np.array([int(i) for i in y_train])
y_train = [y_train.tolist()]
y_train = one_hot_vectors(y_train, num_bins)

y_test = np.array([int(i) for i in y_test])
y_test = [y_test.tolist()]
y_test = one_hot_vectors(y_test, num_bins)

y_val = np.array([int(i) for i in y_val])
y_val = [y_val.tolist()]
y_val = one_hot_vectors(y_val, num_bins)

# Define and configure ImageDataGenerator for training data.
train_datagen = ImageDataGenerator(
    width_shift_range = 0.20)

# Define and configure ImageDataGenerator for validation data.
val_datagen = ImageDataGenerator()

#'''
# Code for displaying the augemented image from train_datagen.

for X_batch, y_batch in train_datagen.flow(X_train, y_train,
                                           batch_size = 9):
  
	# create a grid of 3x3 images
	for i in range(0, 9):
		plt.subplot(330 + 1 + i)
		plt.imshow(X_batch[i].reshape(80, 320, 3))
    
	# show the plot
	plt.show()
	break
#'''

# Define the batch size by calling the flow function.
train_generator = train_datagen.flow(X_train , y_train , batch_size = 64)
val_generator = val_datagen.flow(X_val , y_val , batch_size = 64)

# Modified NVIDIA model

model_cat = models.Sequential()

model_cat.add(layers.Lambda(lambda x: x, input_shape=(80,320,3)))
model_cat.add(layers.Conv2D(24, 5, 5, activation='relu', subsample=(2, 2)))
model_cat.add(layers.Conv2D(36, 5, 5, activation='relu', subsample=(2, 2)))
model_cat.add(layers.Conv2D(48, 5, 5, activation='relu', subsample=(2, 2)))
model_cat.add(layers.Conv2D(64, 3, 3, activation='relu'))
model_cat.add(layers.Conv2D(64, 3, 3, activation='relu'))
model_cat.add(layers.Flatten())
model_cat.add(layers.Dense(500, activation='relu'))
model_cat.add(layers.Dense(400, activation='relu'))
model_cat.add(layers.Dense(300, activation='relu'))
model_cat.add(layers.Dense(200, activation = 'softmax'))

model_cat.compile(optimizer = optimizers.RMSprop(lr=1e-5),
                  loss='categorical_crossentropy',
                  metrics=['acc'])

model_cat.summary()

# Checkpoint

parameterPath = dirName + "/nvidiaLogisticRegression/" + "model_{epoch:02d}-{val_acc:.5f}.h5"

checkpoint = ModelCheckpoint(parameterPath,
                             monitor='val_acc',
                             verbose=1,
                             save_best_only = True,
                             mode='max')

callbacks_list = [checkpoint]

with tf.device('/device:GPU:0'):
    history = model_cat.fit_generator(train_generator,
                                      steps_per_epoch = len(X_train)/10,
                                      validation_steps = 50,
                                      validation_data = val_generator,
                                      shuffle = True,
                                      epochs = 10,
                                      callbacks = callbacks_list,
                                      verbose = 1)

# Extract accuracy and loss for plotting.
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.figure()
plt.plot(epochs, acc, 'm', label='Training Accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')
plt.title('Training and Validation Accuracy' , size = 20)
plt.xlabel('Number of Epochs' , size = 20)
plt.ylabel('Accuracy (Fraction)' , size = 20)
plt.legend()

plt.figure()
plt.plot(epochs, loss, 'm', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('Training and Validation Loss' , size = 20)
plt.xlabel('Number of Epochs', size = 20)
plt.ylabel('Loss' , size = 20)
plt.legend()

plt.show()


